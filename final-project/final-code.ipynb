{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221\n",
      "196\n",
      "198607\n",
      "12000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# 训练集中的作者论文信息\n",
    "with open(\"../cna_data/train_author.json\", \"r\") as f2:\n",
    "    author_data = json.load(f2)\n",
    "\n",
    "# 训练集的论文元信息\n",
    "with open(\"../cna_data/train_pub.json\", \"r\") as f2:\n",
    "    pubs_dict = json.load(f2)\n",
    "\n",
    "print(len(author_data))\n",
    "\n",
    "\n",
    "name_train = set()\n",
    "\n",
    "# 筛选训练集，只取同名作者数大于等于5个的名字作为训练集。\n",
    "for name in author_data:\n",
    "    persons = author_data[name]\n",
    "    if(len(persons) > 5):\n",
    "        name_train.add((name))\n",
    "\n",
    "print(len(name_train))\n",
    "\n",
    "# 采样500个训练例子，一个训练例子包含paper和正例作者以及5个负例作者（正负例比=1：5）\n",
    "\n",
    "# 记录paper所属作者和名字\n",
    "paper2aid2name = {}\n",
    "\n",
    "for author_name in name_train:\n",
    "    persons = author_data[author_name]\n",
    "    for person in persons:\n",
    "        paper_list = persons[person]\n",
    "        for paper_id in paper_list:\n",
    "            paper2aid2name[paper_id] = (author_name, person)\n",
    "\n",
    "print(len(paper2aid2name))\n",
    "# print(paper2aid2name)\n",
    "\n",
    "total_paper_list = list(paper2aid2name.keys())\n",
    "\n",
    "# 采样10000篇paper作为训练集\n",
    "train_paper_list = random.sample(total_paper_list, 12000)\n",
    "\n",
    "# 把采样的500篇paper转变成对应的训练例子，一个训练例子包含paper和正例作者以及5个负例作者（正负例比=1：5）\n",
    "train_instances = []\n",
    "for paper_id in train_paper_list:\n",
    "\n",
    "    # 保存对应的正负例\n",
    "    pos_ins = set()\n",
    "    neg_ins = set()\n",
    "\n",
    "    paper_author_name = paper2aid2name[paper_id][0]\n",
    "    paper_author_id = paper2aid2name[paper_id][1]\n",
    "\n",
    "    pos_ins.add((paper_id, paper_author_id))\n",
    "\n",
    "    # 获取同名的所有作者(除了本身)作为负例的candidate\n",
    "    persons = list(author_data[paper_author_name].keys())\n",
    "    persons.remove(paper_author_id)\n",
    "    assert len(persons) == (len(list(author_data[paper_author_name].keys())) - 1)\n",
    "\n",
    "    # 每个正例采样5个负例\n",
    "    neg_author_list = random.sample(persons, 5)\n",
    "    for i in neg_author_list:\n",
    "        neg_ins.add((paper_id, i))\n",
    "\n",
    "    train_instances.append((pos_ins, neg_ins))\n",
    "\n",
    "print(len(train_instances))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyjarowinkler import distance\n",
    "\n",
    "\n",
    "def clean_name(name):\n",
    "    if name is None:\n",
    "        return \"\"\n",
    "    x = [k.strip() for k in name.lower().strip().replace(\".\", \"\").replace(\"-\", \" \").replace(\"_\", ' ').split()]\n",
    "    full_name = ' '.join(x)\n",
    "    name_part = full_name.split()\n",
    "    if(len(name_part) >= 1):\n",
    "        return full_name\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def delete_main_name(author_list, name):\n",
    "    score_list = []\n",
    "    name = clean_name(name)\n",
    "    author_list_lower = []\n",
    "    for author in author_list:\n",
    "        author_list_lower.append(author.lower())\n",
    "    name_split = name.split()\n",
    "    for author in author_list_lower:\n",
    "        score = distance.get_jaro_distance(name, author, winkler=True, scaling=0.1)\n",
    "        author_split = author.split()\n",
    "        inter = set(name_split) & set(author_split)\n",
    "        alls = set(name_split) | set(author_split)\n",
    "        score += round(len(inter)/len(alls), 6)\n",
    "        score_list.append(score)\n",
    "\n",
    "    rank = np.argsort(-np.array(score_list))\n",
    "    return_list = [author_list_lower[i] for i in rank[1:]]\n",
    "\n",
    "    return return_list, rank[0]\n",
    "\n",
    "def process_feature(pos_ins, paper_coauthors):\n",
    "\n",
    "    feature_list = []\n",
    "\n",
    "    paper = pos_ins[0]\n",
    "    author = pos_ins[1]\n",
    "\n",
    "\n",
    "    author_name = paper2aid2name[paper][0]\n",
    "    doc_list = []\n",
    "    for doc in author_data[author_name][author]:\n",
    "        if(doc != paper):\n",
    "            doc_list.append(doc)\n",
    "    for doc in doc_list:\n",
    "        if doc == paper:\n",
    "            print(\"error!\")\n",
    "            exit()\n",
    "\n",
    "    candidate_authors_int = defaultdict(int)\n",
    "\n",
    "    total_author_count = 0\n",
    "    for doc in doc_list:\n",
    "\n",
    "        doc_dict = pubs_dict[doc]\n",
    "        author_list = []\n",
    "\n",
    "        paper_authors = doc_dict['authors']\n",
    "        paper_authors_len = len(paper_authors)\n",
    "        paper_authors = random.sample(paper_authors, min(50, paper_authors_len))\n",
    "\n",
    "        for author in paper_authors:                \n",
    "            clean_author = clean_name(author['name'])\n",
    "            if(clean_author != None):\n",
    "                author_list.append(clean_author)\n",
    "        if(len(author_list) > 0):\n",
    "            _, author_index = delete_main_name(author_list, author_name)\n",
    "\n",
    "            for index in range(len(author_list)):\n",
    "                if(index == author_index):\n",
    "                    continue\n",
    "                else:\n",
    "                    candidate_authors_int[author_list[index]] += 1\n",
    "                    total_author_count += 1\n",
    "\n",
    "    author_keys = list(candidate_authors_int.keys())\n",
    "    #  指标\n",
    "    #  p1 = 目标论文的重复合作者数 / 目标论文的总作者数\n",
    "    #  p2 = 目标论文的合作者重复的总次数 / 目标论文的合作者总数\n",
    "\n",
    "    if ((len(author_keys) == 0) or (len(paper_coauthors) == 0)):\n",
    "        feature_list.extend([0.] * 5)\n",
    "    else:\n",
    "        co_coauthors = set(paper_coauthors) & set(author_keys)\n",
    "\n",
    "        co_coauthors_ratio_for_author = round(coauthor_len / len(author_keys), 6)\n",
    "\n",
    "        coauthor_count = 0\n",
    "        for coauthor_name in co_coauthors:\n",
    "            coauthor_count += candidate_authors_int[coauthor_name]\n",
    "\n",
    "        co_coauthors_ratio_for_author_count = round(coauthor_count / total_author_count, 6)\n",
    "        feature_list.extend([co_coauthors_ratio_for_author, co_coauthors_ratio_for_author_count])\n",
    "\n",
    "    return feature_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n",
      "(11997, 5)\n",
      "(59985, 5)\n"
     ]
    }
   ],
   "source": [
    "#生成所有正例以及负例的特征\n",
    "from collections import defaultdict\n",
    "\n",
    "pos_features = []\n",
    "neg_features = []\n",
    "\n",
    "print(len(train_instances))\n",
    "\n",
    "for ins in train_instances:\n",
    "\n",
    "    pos_set = ins[0]\n",
    "    neg_set = ins[1]\n",
    "    paper_id = list(pos_set)[0][0]\n",
    "    author_name = paper2aid2name[paper_id][0]\n",
    "\n",
    "\n",
    "    author_list = []\n",
    "    paper_coauthors = []\n",
    "\n",
    "\n",
    "    paper_authors = pubs_dict[paper_id]['authors']\n",
    "    paper_authors_len = len(paper_authors)\n",
    "    paper_authors = random.sample(paper_authors, min(50, paper_authors_len))\n",
    "\n",
    "    for author in paper_authors:                \n",
    "        clean_author = clean_name(author['name'])\n",
    "        if(clean_author != None):\n",
    "            author_list.append(clean_author)\n",
    "    if(len(author_list) > 0):\n",
    "        _, author_index = delete_main_name(author_list, author_name)\n",
    "\n",
    "        for index in range(len(author_list)):\n",
    "            if(index == author_index):\n",
    "                continue\n",
    "            else:\n",
    "                paper_coauthors.append(author_list[index])\n",
    "\n",
    "\n",
    "        for pos_ins in pos_set:\n",
    "            pos_features.append(process_feature(pos_ins, paper_coauthors))\n",
    "\n",
    "        for neg_ins in neg_set:\n",
    "            neg_features.append(process_feature(neg_ins, paper_coauthors))\n",
    "\n",
    "print(np.array(pos_features).shape)\n",
    "print(np.array(neg_features).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71982, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='rbf', max_iter=-1, probability=True, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# 构建svm正负例\n",
    "svm_train_ins = []\n",
    "for ins in pos_features:\n",
    "    svm_train_ins.append((ins, 1))\n",
    "\n",
    "for ins in neg_features:\n",
    "    svm_train_ins.append((ins, 0))\n",
    "\n",
    "random.shuffle(svm_train_ins)\n",
    "\n",
    "x_train= []\n",
    "y_train = []\n",
    "for ins in svm_train_ins:\n",
    "    x_train.append(ins[0])\n",
    "    y_train.append(ins[1])\n",
    "\n",
    "clf = SVC(probability=True)\n",
    "clf.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320\n",
      "9650\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "6298\n",
      "6281\n"
     ]
    }
   ],
   "source": [
    "# 已有用户档案\n",
    "with open(\"../cna_data/whole_author_profile.json\", \"r\") as f2:\n",
    "    test_author_data = json.load(f2)\n",
    "\n",
    "with open(\"../cna_data/whole_author_profile_pub.json\", \"r\") as f2:\n",
    "    test_pubs_dict = json.load(f2)\n",
    "\n",
    "# 待分配论文集\n",
    "with open(\"../cna_data/cna_test_data/cna_test_unass_competition.json\", \"r\") as f2:\n",
    "# with open(\"../cna_data/cna_valid_unass_competition.json\", \"r\") as f2:\n",
    "    unass_papers = json.load(f2)\n",
    "\n",
    "with open(\"../cna_data/cna_test_data/cna_test_pub.json\", \"r\") as f2:\n",
    "# with open(\"../cna_data/cna_valid_pub.json\", \"r\") as f2:\n",
    "    unass_papers_dict = json.load(f2)\n",
    "\n",
    "\n",
    "# 简单处理whole_author_profile，将同名的作者合并：\n",
    "# 为了效率，预处理new_test_author_data中的paper，将其全部处理成paper_id + '-' + author_index的形式。\n",
    "new_test_author_data = {}\n",
    "for author_id, author_info in test_author_data.items():\n",
    "    author_name = author_info['name']\n",
    "    author_papers = author_info['papers']\n",
    "    newly_papers = []\n",
    "\n",
    "    for paper_id in author_papers:\n",
    "\n",
    "        paper_authors = test_pubs_dict[paper_id]['authors']\n",
    "        paper_authors_len = len(paper_authors)\n",
    "\n",
    "        if(paper_authors_len > 50):\n",
    "            continue\n",
    "        author_list = []\n",
    "        for author in paper_authors:                \n",
    "            clean_author = clean_name(author['name'])\n",
    "            if(clean_author != None):\n",
    "                author_list.append(clean_author)\n",
    "        if(len(author_list) > 0):\n",
    "            _, author_index = delete_main_name(author_list, clean_name(author_name))\n",
    "\n",
    "            new_paper_id = str(paper_id) + '-' + str(author_index)\n",
    "            newly_papers.append(new_paper_id)\n",
    "\n",
    "\n",
    "    if(new_test_author_data.get(author_name) != None):\n",
    "        new_test_author_data[author_name][author_id] = newly_papers\n",
    "    else:\n",
    "        tmp = {}\n",
    "        tmp[author_id] = newly_papers\n",
    "        new_test_author_data[author_name] = tmp\n",
    "print(len(new_test_author_data))\n",
    "\n",
    "# test集的特征生成函数\n",
    "def process_test_feature(pair, new_test_author_data, test_pubs_dict, paper_coauthors):\n",
    "\n",
    "    feature_list = []\n",
    "\n",
    "    paper = pair[0]\n",
    "    author = pair[1]\n",
    "    paper_au_name = pair[2]\n",
    "\n",
    "    doc_list = new_test_author_data[paper_au_name][author]\n",
    "\n",
    "\n",
    "    # 保存作者的所有coauthors以及各自出现的次数(作者所拥有论文的coauthors)\n",
    "    candidate_authors_int = defaultdict(int)\n",
    "\n",
    "    total_author_count = 0\n",
    "    for doc in doc_list:\n",
    "        doc_id = doc.split('-')[0]\n",
    "        author_index = doc.split('-')[1]\n",
    "        doc_dict = test_pubs_dict[doc_id]\n",
    "        author_list = []\n",
    "\n",
    "        paper_authors = doc_dict['authors']\n",
    "        paper_authors_len = len(paper_authors)\n",
    "        paper_authors = random.sample(paper_authors, min(50, paper_authors_len))\n",
    "\n",
    "        for author in paper_authors:                \n",
    "            clean_author = clean_name(author['name'])\n",
    "            if(clean_author != None):\n",
    "                author_list.append(clean_author)\n",
    "        if(len(author_list) > 0):\n",
    "\n",
    "            # 获取除了main author_name外的coauthor\n",
    "            for index in range(len(author_list)):\n",
    "                if(index == author_index):\n",
    "                    continue\n",
    "                else:\n",
    "                    candidate_authors_int[author_list[index]] += 1\n",
    "                    total_author_count += 1\n",
    "\n",
    "    author_keys = list(candidate_authors_int.keys())\n",
    "\n",
    "    if ((len(author_keys) == 0) or (len(paper_coauthors) == 0)):\n",
    "        feature_list.extend([0.] * 5)\n",
    "    else:\n",
    "        co_coauthors = set(paper_coauthors) & set(author_keys)\n",
    "        coauthor_len = len(co_coauthors)\n",
    "\n",
    "\n",
    "        co_coauthors_ratio_for_author = round(coauthor_len / len(author_keys), 6)\n",
    "\n",
    "        coauthor_count = 0\n",
    "        for coauthor_name in co_coauthors:\n",
    "            coauthor_count += candidate_authors_int[coauthor_name]\n",
    "\n",
    "\n",
    "\n",
    "        co_coauthors_ratio_for_author_count = round(coauthor_count / total_author_count, 6)\n",
    "\n",
    "           #  指标\n",
    "    #  p1 = 目标论文的重复合作者数 / 目标论文的总作者数\n",
    "    #  p2 = 目标论文的合作者重复的总次数 / 目标论文的合作者总数\n",
    "        feature_list.extend([co_coauthors_ratio_for_author, co_coauthors_ratio_for_author_count])\n",
    "\n",
    "    return feature_list\n",
    "print(len(unass_papers))\n",
    "\n",
    "\n",
    "count = 0\n",
    "\n",
    "# 存储paper的所有candidate author id\n",
    "paper2candidates = defaultdict(list)\n",
    "# 存储对应的paper与candidate author的生成特征\n",
    "paper2features = defaultdict(list)\n",
    "\n",
    "for u_p in unass_papers:\n",
    "    paper_id = u_p.split('-')[0]\n",
    "    author_index = int(u_p.split('-')[1])\n",
    "    author_list = []\n",
    "\n",
    "    # 获取paper的coauthors\n",
    "    paper_coauthors = []\n",
    "    paper_au_name = ''\n",
    "    paper_authors = unass_papers_dict[paper_id]['authors']\n",
    "#     paper_authors_len = len(paper_authors)\n",
    "#     paper_authors = random.sample(paper_authors, min(50, paper_authors_len))\n",
    "\n",
    "    for author in paper_authors:                \n",
    "        clean_author = clean_name(author['name'])\n",
    "        if(clean_author != None):\n",
    "            author_list.append(clean_author)\n",
    "    if(len(author_list) > 0):\n",
    "\n",
    "        # 获取除了main author_name外的coauthor\n",
    "        for index in range(len(author_list)):\n",
    "            if(index == author_index):\n",
    "                continue\n",
    "            else:\n",
    "                paper_coauthors.append(author_list[index])\n",
    "\n",
    "    # 简单使用精确匹配找出candidate_author_list\n",
    "    if(clean_name(paper_authors[author_index]['name']) != None):\n",
    "        paper_au_name = '_'.join(clean_name(paper_authors[author_index]['name']).split())\n",
    "        if(new_test_author_data.get(paper_au_name) != None):\n",
    "            candidate_author_list = new_test_author_data[paper_au_name]\n",
    "            for candidate_author in candidate_author_list:\n",
    "                pair = (paper_id, candidate_author, paper_au_name)\n",
    "                paper2candidates[paper_id].append(candidate_author)\n",
    "                paper2features[paper_id].append(process_test_feature(pair, new_test_author_data, test_pubs_dict, paper_coauthors))\n",
    "            count += 1\n",
    "            if(count%1000 == 0):\n",
    "                print(count)\n",
    "print(count)\n",
    "assert len(paper2candidates) == len(paper2features)\n",
    "print(len(paper2candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = defaultdict(list)\n",
    "for paper_id, ins_feature_list in paper2features.items():\n",
    "    score_list = []\n",
    "    for ins in ins_feature_list:\n",
    "        # 利用svm对所有co-author去打分，利用分数进行排序，取top-1作为预测的author\n",
    "        prob_pred = clf.predict_proba([ins])[:, 1]\n",
    "        score_list.append(prob_pred[0])\n",
    "    rank = np.argsort(-np.array(score_list))\n",
    "    predict_author = paper2candidates[paper_id][rank[0]]\n",
    "    result_dict[predict_author].append(paper_id)\n",
    "\n",
    "with open(\"../cna_data/result.json\", 'w') as files:\n",
    "    json.dump(result_dict, files, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
