{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering text documents "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 10,
>>>>>>> ed9855c0300ab93a13f5ff7cdea7ed9d441c7af2
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "Usage: ipykernel_launcher.py [options]\n",
      "\n",
      "Options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --lsa=N_COMPONENTS    Preprocess documents with latent semantic analysis.\n",
      "  --no-minibatch        Use ordinary k-means algorithm (in batch mode).\n",
      "  --no-idf              Disable Inverse Document Frequency feature weighting.\n",
      "  --use-hashing         Use a hashing feature vectorizer\n",
      "  --n-features=N_FEATURES\n",
      "                        Maximum number of features (dimensions) to extract\n",
      "                        from text.\n",
      "  --verbose             Print progress reports inside k-means algorithm.\n",
      "Loading 20 newsgroups dataset for categories:\n",
      "['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
      "3387 documents\n",
      "4 categories\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
    "#         Lars Buitinck\n",
    "# License: BSD 3 clause\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "# parse commandline arguments\n",
    "op = OptionParser()\n",
    "op.add_option(\"--lsa\",\n",
    "              dest=\"n_components\", type=\"int\",\n",
    "              help=\"Preprocess documents with latent semantic analysis.\")\n",
    "op.add_option(\"--no-minibatch\",\n",
    "              action=\"store_false\", dest=\"minibatch\", default=True,\n",
    "              help=\"Use ordinary k-means algorithm (in batch mode).\")\n",
    "op.add_option(\"--no-idf\",\n",
    "              action=\"store_false\", dest=\"use_idf\", default=True,\n",
    "              help=\"Disable Inverse Document Frequency feature weighting.\")\n",
    "op.add_option(\"--use-hashing\",\n",
    "              action=\"store_true\", default=False,\n",
    "              help=\"Use a hashing feature vectorizer\")\n",
    "op.add_option(\"--n-features\", type=int, default=10000,\n",
    "              help=\"Maximum number of features (dimensions)\"\n",
    "                   \" to extract from text.\")\n",
    "op.add_option(\"--verbose\",\n",
    "              action=\"store_true\", dest=\"verbose\", default=False,\n",
    "              help=\"Print progress reports inside k-means algorithm.\")\n",
    "\n",
    "print(__doc__)\n",
    "op.print_help()\n",
    "\n",
    "\n",
    "def is_interactive():\n",
    "    return not hasattr(sys.modules['__main__'], '__file__')\n",
    "\n",
    "\n",
    "# work-around for Jupyter notebook and IPython console\n",
    "argv = [] if is_interactive() else sys.argv[1:]\n",
    "(opts, args) = op.parse_args(argv)\n",
    "if len(args) > 0:\n",
    "    op.error(\"this script takes no arguments.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Load some categories from the training set\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "# Uncomment the following to do the analysis on all the categories\n",
    "# categories = None\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)\n",
    "\n",
    "dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
    "                             shuffle=True, random_state=42)\n",
    "\n",
    "print(\"%d documents\" % len(dataset.data))\n",
    "print(\"%d categories\" % len(dataset.target_names))\n",
    "print()\n",
    "\n",
    "labels = dataset.target\n",
    "true_k = np.unique(labels).shape[0]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 11,
>>>>>>> ed9855c0300ab93a13f5ff7cdea7ed9d441c7af2
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from the training dataset using a sparse vectorizer\n",
<<<<<<< HEAD
      "done in 0.720953s\n",
=======
      "done in 0.798557s\n",
>>>>>>> ed9855c0300ab93a13f5ff7cdea7ed9d441c7af2
      "n_samples: 3387, n_features: 10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 将文本转化为tf-idf\n",
    "print(\"Extracting features from the training dataset \"\n",
    "      \"using a sparse vectorizer\")\n",
    "t0 = time()\n",
    "if opts.use_hashing:\n",
    "    if opts.use_idf:\n",
    "        hasher = HashingVectorizer(n_features=opts.n_features,\n",
    "                                   stop_words='english', alternate_sign=False,\n",
    "                                   norm=None, binary=False)\n",
    "        vectorizer = make_pipeline(hasher, TfidfTransformer())\n",
    "    else:\n",
    "        vectorizer = HashingVectorizer(n_features=opts.n_features,\n",
    "                                       stop_words='english',\n",
    "                                       alternate_sign=False, norm='l2',\n",
    "                                       binary=False)\n",
    "else:\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, max_features=opts.n_features,\n",
    "                                 min_df=2, stop_words='english',\n",
    "                                 use_idf=opts.use_idf)\n",
    "X = vectorizer.fit_transform(dataset.data)\n",
    "\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "print()\n",
    "\n",
    "if opts.n_components:\n",
    "    print(\"Performing dimensionality reduction using LSA\")\n",
    "    t0 = time()\n",
    "    svd = TruncatedSVD(opts.n_components)\n",
    "    normalizer = Normalizer(copy=False)\n",
    "    lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "    X = lsa.fit_transform(X)\n",
    "\n",
    "    print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "    explained_variance = svd.explained_variance_ratio_.sum()\n",
    "    print(\"Explained variance of the SVD step: {}%\".format(\n",
    "        int(explained_variance * 100)))\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sparse data with MiniBatchKMeans(batch_size=1000, compute_labels=True, init='k-means++',\n",
      "                init_size=1000, max_iter=100, max_no_improvement=10,\n",
      "                n_clusters=4, n_init=1, random_state=None,\n",
      "                reassignment_ratio=0.01, tol=0.0, verbose=False)\n",
<<<<<<< HEAD
      "done in 0.219s\n",
      "\n",
      "NMI: 0.537\n",
      "Homogeneity: 0.514\n",
      "Completeness: 0.560\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\cluster\\supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
=======
      "done in 0.142s\n",
      "\n",
      "Homogeneity: 0.599\n",
      "Completeness: 0.636\n",
      "V-measure: 0.617\n",
      "Adjusted Rand-Index: 0.622\n",
      "Silhouette Coefficient: 0.008\n",
      "\n",
      "Top terms per cluster:\n",
      "Cluster 0: graphics university image com thanks files 3d file posting host\n",
      "Cluster 1: god com sandvik jesus people religion don bible article believe\n",
      "Cluster 2: space nasa henry access digex com toronto gov pat alaska\n",
      "Cluster 3: sgi keith livesey morality objective caltech com moral jon wpd\n"
>>>>>>> ed9855c0300ab93a13f5ff7cdea7ed9d441c7af2
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "if opts.minibatch:\n",
    "    km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1,\n",
    "                         init_size=1000, batch_size=1000, verbose=opts.verbose)\n",
    "else:\n",
    "    km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1,\n",
    "                verbose=opts.verbose)\n",
    "\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "t0 = time()\n",
    "km.fit(X)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
<<<<<<< HEAD
    "print(\"NMI: %0.3f\" % metrics.normalized_mutual_info_score(labels, km.labels_))\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "\n",
    "print()\n"
=======
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "if not opts.use_hashing:\n",
    "    print(\"Top terms per cluster:\")\n",
    "\n",
    "    if opts.n_components:\n",
    "        original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "        order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "    else:\n",
    "        order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    for i in range(true_k):\n",
    "        print(\"Cluster %d:\" % i, end='')\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' %s' % terms[ind], end='')\n",
    "        print()"
>>>>>>> ed9855c0300ab93a13f5ff7cdea7ed9d441c7af2
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MeanShift"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 12,
>>>>>>> ed9855c0300ab93a13f5ff7cdea7ed9d441c7af2
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "done in 42.911s\n",
      "\n",
      "NMI: 0.537\n",
      "Homogeneity: 0.514\n",
      "Completeness: 0.560\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\cluster\\supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
=======
      "done in 45.410s\n",
      "\n",
      "Homogeneity: 0.000\n",
      "Completeness: 1.000\n",
      "V-measure: 0.000\n",
      "Adjusted Rand-Index: 0.000\n",
      "\n"
     ]
>>>>>>> ed9855c0300ab93a13f5ff7cdea7ed9d441c7af2
    }
   ],
   "source": [
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "\n",
    "X_array = X.toarray()\n",
    "\n",
    "bandwidth = estimate_bandwidth(X_array, quantile = 0.1, n_samples = 500)\n",
    "ms = MeanShift(bandwidth = bandwidth, bin_seeding = True)\n",
    "\n",
    "t0 = time()\n",
    "ms.fit(X_array)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
<<<<<<< HEAD
    "print(\"NMI: %0.3f\" % metrics.normalized_mutual_info_score(labels, km.labels_))\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "\n",
=======
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, ms.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, ms.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, ms.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, ms.labels_))\n",
>>>>>>> ed9855c0300ab93a13f5ff7cdea7ed9d441c7af2
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. affinity propagation"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 14,
>>>>>>> ed9855c0300ab93a13f5ff7cdea7ed9d441c7af2
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "done in 39.017s\n",
      "NMI: 0.537\n",
      "Homogeneity: 0.514\n",
      "Completeness: 0.560\n"
=======
      "done in 41.243s\n",
      "Homogeneity: 1.000\n",
      "Completeness: 0.169\n",
      "V-measure: 0.289\n",
      "Adjusted Rand-Index: 0.000\n"
>>>>>>> ed9855c0300ab93a13f5ff7cdea7ed9d441c7af2
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "\n",
    "af = AffinityPropagation(preference=-200)\n",
    "t0 = time()\n",
    "af.fit(X)\n",
    "\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
<<<<<<< HEAD
    "print(\"NMI: %0.3f\" % metrics.normalized_mutual_info_score(labels, km.labels_))\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "\n"
=======
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, af.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, af.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, af.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, af.labels_))\n"
>>>>>>> ed9855c0300ab93a13f5ff7cdea7ed9d441c7af2
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.SpectralClustering"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 17,
>>>>>>> ed9855c0300ab93a13f5ff7cdea7ed9d441c7af2
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "done in 40.485s\n",
      "NMI: 0.537\n",
      "Homogeneity: 0.514\n",
      "Completeness: 0.560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\cluster\\supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
=======
      "done in 149.257s\n",
      "Homogeneity: 0.082\n",
      "Completeness: 0.263\n",
      "V-measure: 0.125\n",
      "Adjusted Rand-Index: 0.020\n"
>>>>>>> ed9855c0300ab93a13f5ff7cdea7ed9d441c7af2
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "\n",
    "sc = SpectralClustering(n_clusters=true_k)\n",
    "sc.fit(X)\n",
    "\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
<<<<<<< HEAD
    "print(\"NMI: %0.3f\" % metrics.normalized_mutual_info_score(labels, km.labels_))\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "\n"
=======
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, sc.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, sc.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, sc.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, sc.labels_))\n"
>>>>>>> ed9855c0300ab93a13f5ff7cdea7ed9d441c7af2
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.Ward Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 20,
>>>>>>> ed9855c0300ab93a13f5ff7cdea7ed9d441c7af2
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "done in 104.075s\n",
      "NMI: 0.537\n",
      "Homogeneity: 0.514\n",
      "Completeness: 0.560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\cluster\\supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
=======
      "done in 253.530s\n",
      "Homogeneity: 0.060\n",
      "Completeness: 0.063\n",
      "V-measure: 0.061\n",
      "Adjusted Rand-Index: 0.061\n"
>>>>>>> ed9855c0300ab93a13f5ff7cdea7ed9d441c7af2
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering \n",
    "\n",
    "X_array = X.toarray()\n",
    "\n",
<<<<<<< HEAD
    "ac = AgglomerativeClustering(linkage = 'ward', n_clusters = true_k)\n",
    "ac.fit(X_array)\n",
    "\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print(\"NMI: %0.3f\" % metrics.normalized_mutual_info_score(labels, km.labels_))\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "\n"
=======
    "ac = AgglomerativeClustering(linkage = 'complete', n_clusters = true_k)\n",
    "ac.fit(X_array)\n",
    "\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, ac.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, ac.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, ac.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, ac.labels_))\n"
>>>>>>> ed9855c0300ab93a13f5ff7cdea7ed9d441c7af2
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.DBSCAN"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 21,
>>>>>>> ed9855c0300ab93a13f5ff7cdea7ed9d441c7af2
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "done in 104.510s\n",
      "NMI: 0.537\n",
      "Homogeneity: 0.514\n",
      "Completeness: 0.560\n"
=======
      "done in 314.457s\n",
      "Homogeneity: 0.004\n",
      "Completeness: 0.153\n",
      "V-measure: 0.007\n",
      "Adjusted Rand-Index: 0.000\n"
>>>>>>> ed9855c0300ab93a13f5ff7cdea7ed9d441c7af2
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "dbscan = DBSCAN(eps = 0.1, min_samples = 2)\n",
    "dbscan.fit(X)\n",
    "\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
<<<<<<< HEAD
    "print(\"NMI: %0.3f\" % metrics.normalized_mutual_info_score(labels, km.labels_))\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "\n"
=======
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, dbscan.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, dbscan.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, dbscan.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, dbscan.labels_))\n"
>>>>>>> ed9855c0300ab93a13f5ff7cdea7ed9d441c7af2
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "# 7. Agglomerative clustering\n",
    "分层聚类通过依次合并或拆分簇来构造嵌套的树形簇结构。"
=======
    "# 7.OPTICS"
>>>>>>> ed9855c0300ab93a13f5ff7cdea7ed9d441c7af2
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 168.609s\n",
      "NMI: 0.537\n",
      "Homogeneity: 0.514\n",
      "Completeness: 0.560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\cluster\\supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
=======
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-0e0cf80d799e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0moptics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOPTICS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0moptics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"done in %0.3fs\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\sklearn\\cluster\\optics_.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    246\u001b[0m              \u001b[0mleaf_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleaf_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m              \u001b[0mmetric_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetric_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m              max_eps=self.max_eps)\n\u001b[0m\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;31m# Extract clusters from the calculated orders and reachability\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\sklearn\\cluster\\optics_.py\u001b[0m in \u001b[0;36mcompute_optics_graph\u001b[1;34m(X, min_samples, max_eps, metric, p, metric_params, algorithm, leaf_size, n_jobs)\u001b[0m\n\u001b[0;32m    460\u001b[0m     core_distances_ = _compute_core_distances_(X=X, neighbors=nbrs,\n\u001b[0;32m    461\u001b[0m                                                \u001b[0mmin_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m                                                working_memory=None)\n\u001b[0m\u001b[0;32m    463\u001b[0m     \u001b[1;31m# OPTICS puts an upper limit on these, use inf for undefined.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[0mcore_distances_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcore_distances_\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mmax_eps\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\sklearn\\cluster\\optics_.py\u001b[0m in \u001b[0;36m_compute_core_distances_\u001b[1;34m(X, neighbors, min_samples, working_memory)\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m         core_distances[sl] = neighbors.kneighbors(\n\u001b[1;32m--> 327\u001b[1;33m             X[sl], min_samples)[0][:, -1]\n\u001b[0m\u001b[0;32m    328\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcore_distances\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\sklearn\\neighbors\\base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    452\u001b[0m                 delayed_query(\n\u001b[0;32m    453\u001b[0m                     self._tree, X[s], n_neighbors, return_distance)\n\u001b[1;32m--> 454\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m             )\n\u001b[0;32m    456\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    919\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\sklearn\\neighbors\\base.py\u001b[0m in \u001b[0;36m_tree_query_parallel_helper\u001b[1;34m(tree, data, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    289\u001b[0m     \u001b[0munder\u001b[0m \u001b[0mPyPy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m     \"\"\"\n\u001b[1;32m--> 291\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
>>>>>>> ed9855c0300ab93a13f5ff7cdea7ed9d441c7af2
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "from sklearn.cluster import AgglomerativeClustering \n",
    "\n",
    "X_array = X.toarray()\n",
    "\n",
    "ac = AgglomerativeClustering(linkage = 'complete', n_clusters = true_k)\n",
    "ac.fit(X_array)\n",
    "\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print(\"NMI: %0.3f\" % metrics.normalized_mutual_info_score(labels, km.labels_))\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "\n"
=======
    "from sklearn.cluster import OPTICS\n",
    "\n",
    "X_array = X.toarray()\n",
    "\n",
    "optics = OPTICS(eps=0.01, min_samples=2)\n",
    "optics.fit(X_array)\n",
    "\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, optics.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, optics.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, optics.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, optics.labels_))\n"
>>>>>>> ed9855c0300ab93a13f5ff7cdea7ed9d441c7af2
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.Gaussian mixtures"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 12,
>>>>>>> ed9855c0300ab93a13f5ff7cdea7ed9d441c7af2
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "done in 242.316s\n",
      "NMI: 0.537\n",
      "Homogeneity: 0.514\n",
      "Completeness: 0.560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\cluster\\supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
=======
      "done in 80.013s\n",
      "Homogeneity: 0.000\n",
      "Completeness: 1.000\n",
      "V-measure: 0.000\n",
      "Adjusted Rand-Index: 0.000\n"
>>>>>>> ed9855c0300ab93a13f5ff7cdea7ed9d441c7af2
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "X_array = X.toarray()\n",
    "gm = GaussianMixture()\n",
    "gm.fit(X_array)\n",
    "labels_ = gm.predict(X_array)\n",
    "\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
<<<<<<< HEAD
    "print(\"NMI: %0.3f\" % metrics.normalized_mutual_info_score(labels, km.labels_))\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "\n"
   ]
=======
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, labels_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.Birch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 208.593s\n",
      "Homogeneity: 0.479\n",
      "Completeness: 0.555\n",
      "V-measure: 0.514\n",
      "Adjusted Rand-Index: 0.488\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import Birch\n",
    "\n",
    "birch = Birch(n_clusters = true_k)\n",
    "birch.fit(X)\n",
    "\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, birch.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, birch.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, birch.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, birch.labels_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
>>>>>>> ed9855c0300ab93a13f5ff7cdea7ed9d441c7af2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
